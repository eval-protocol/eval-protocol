---
title: ğœÂ²-bench â€” Retail
description: 'Multi-turn retail environment evaluation with MCP tool interactions and comprehensive reward scoring'
sidebarTitle: ğœÂ²-bench (Retail)
---

This example demonstrates a multi-turn retail customer service evaluation using ğœÂ²-bench environments and MCP tool interactions. For a detailed walkthrough of the concepts behind this evaluation, see our [Multi-Turn Evaluation with User Simulation tutorial](/tutorial/multi-turn-eval-user-simulation).

<Note>
You can find the complete implementation in the Python SDK at `tests/pytest/test_tau_bench_retail.py` and exported as `tau_bench_retail`.
</Note>

## What it does

- Uses multi-turn conversations with MCP tool calling in a retail environment
- Evaluates agents across database state validation and communication quality
- Applies multiplicative scoring where all criteria must pass for full credit
- Runs simulated customer service scenarios with realistic tool interactions

## How it's configured

- `@evaluation_test` uses `MCPGymRolloutProcessor` for multi-turn tool interactions
- Retail dataset entries include evaluation criteria and user simulation contexts
- ğœÂ²-bench reward system validates environment state changes and communication quality

## Run it locally

From the Python SDK repo root:

```bash
python -m eval_protocol.benchmarks.run tau_bench_retail \
  --model fireworks_ai/accounts/fireworks/models/gpt-oss-120b \
  --print-summary --out artifacts/
```

Use `--ep-num-runs 8` and `--max-concurrency 64` for efficient multiple evaluation runs to assess consistency across the stochastic multi-turn interactions.

## Notes

- This evaluation involves multi-turn conversations with tool calling, making it computationally intensive
- Multiple runs recommended due to the stochastic nature of multi-turn user simulation
- Final score uses multiplicative reward where all evaluation criteria must pass for full credit
