---
title: AIME 2025 (Open-Resource)
description: 'Quick AIME-style math check using boxed final answers'
sidebarTitle: AIME 2025
---

This example wires up a lightweight AIME-style evaluation using the open `AIME2025` JSONL from Hugging Face. It is intended for quick model picking rather than a full reimplementation of the benchmark.

<Note>
Code lives in the Python SDK at examples/aime2025_chat_completion. See the test at [examples/aime2025_chat_completion/tests/test_evaluation.py](https://github.com/eval-protocol/python-sdk/blob/main/examples/aime2025_chat_completion/tests/test_evaluation.py).
</Note>

## What it does

- Pulls AIME2025 JSONL directly from Hugging Face.
- Prompts the model to reason and place the final answer inside `\\boxed{...}`.
- Parses the boxed value and compares it against ground truth for exact match scoring.

## How itâ€™s configured

Key pieces in the SDK example:
- Dataset adapter converts raw rows with `question` and `answer` into `EvaluationRow`s.
- `@evaluation_test` provides URLs, model, and rollout parameters (including optional reasoning-effort variants).
- Evaluator extracts a final integer from the assistant message and checks equality with the ground truth.

## Run it locally

From the Python SDK repo root:

```bash
pytest -q examples/aime2025_chat_completion/tests/test_evaluation.py \
  --ep-model fireworks_ai/accounts/fireworks/models/gpt-oss-120b \
  --ep-max-rows 2
```

Tip: remove `--ep-max-rows` to run the full dataset. You can also adjust `rollout_input_params` in the test to try different reasoning-effort settings.

## Notes

- This is a convenience wrapper for model selection, not a canonical reproduction of AIME.
- The evaluation is strict exact match over a parsed integer from `\\boxed{...}`.
