---
title: Principles
sidebarTitle: Principles
---

Eval Protocol (EP) is built on these fundamental principles that guide every design decision and feature:

## 1. **Evaluations as Code**

EP treats evaluations as first-class code—not configuration files or ad-hoc scripts. Every evaluation is a pytest function that can be:
- Parameterized and composed
- Version controlled and tested
- Integrated into CI/CD pipelines
- Reused across different models and datasets

This approach ensures evaluations are maintainable, debuggable, and evolve with your codebase.

## 2. **Developer Experience First**

EP prioritizes developer productivity through:
- **Simple Integration**: Write evals as pytest functions with familiar decorators
- **Rich Metadata**: Automatic parameterization, result storage, and tooling
- **Flexible Data Models**: Good defaults with extensibility for complex scenarios
- **IDE Support**: Full IntelliSense, debugging, and testing integration

## 3. **Standards-Based Interoperability**

EP builds on existing, proven standards rather than creating new ones:
- **OpenAI Chat Completions API**: Compatible with industry-standard model interfaces
- **Model Context Protocol (MCP)**: Leverages established tool-calling standards
- **pytest**: Integrates with the Python testing ecosystem you already know

This ensures EP works with your existing tools and workflows.

## 4. **Non-Prescriptive Architecture**

EP does not prescribe how your AI systems work:
- **Flexible Rollout Processors**: Use default processors for simple LLM calls or LLM + MCP calls, or bring your own custom implementations
- **Custom Integration**: Write rollout processors in Python or call out to external APIs to produce evaluation inputs
- **System Agnostic**: Works with any AI architecture, from simple chat completions to complex multi-agent systems
- **Extensible Design**: Adapt EP to your specific use case rather than adapting your system to EP

This flexibility ensures EP can evaluate any AI system, regardless of its internal architecture or deployment strategy.

## 5. **Performance at Scale**

EP is designed for production workloads:
- **Parallel Execution**: Efficient parallel processing for large evaluation runs
- **Optimized for Multi-turn**: Specialized handling for complex agent evaluations

## 6. **Evolutionary Architecture**

EP grows with your AI development journey:
- **Single-turn to Multi-turn**: Start with simple model comparisons, scale to complex agent evaluations
- **Static to Dynamic**: Begin with curated datasets, evolve to interactive environments
- **Evaluation to Training**: Use the same rubrics for benchmarking and RL dataset generation

## 7. **Reinforcement Learning Ready**

EP is designed to bridge the gap between evaluation and training:
- **Per-step Rewards**: Structured feedback for RL training
- **Environment Simulation**: Realistic agent testing scenarios
- **User Simulation**: Automated interaction testing
- **Data Flywheel**: Turn evaluations into training data

The goal is to help developers build AI systems that improve through feedback loops, not just prompt engineering.

---

**In essence**: EP transforms evaluations from one-off tests into the foundation of your AI development loop—enabling you to build systems that learn and improve over time.