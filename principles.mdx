### Use existing standards

EP relies on existing data models and standards like OpenAI's Chat Completions
API and MCP.

### Great Developer Experience

EP is designed to be easy to use through your IDE as evals are simply pytest
functions. The decorator helps you parameterize your evals, store metadata,
and standardize your evals across teams. The data model has good defaults for
searching through your dataset while being flexible enough to support complex
environments.

### Built for Reusability and Growth

EP is designed so that you can use the same evaluation specification and dataset
format across your entire AI development lifecycleâ€”from simple single-turn evals
to building complex, multi-turn RL datasets. This ensures your work remains
compatible and reusable as your needs evolve.

### Fast

Evals can take a long time to run, especially if you have multi-turn agents.
EP's Python SDK is designed to efficiently run evals in parallel to ensure that
you spend less time waiting for evals to run.

### Supports Multi-turn Agents

EP's Python SDK helps with complex multi-turn agent evals by providing a
structured but flexible way to author multi-turn rollouts with per-step rewards,
user simulation, or environment simulation.

### Integrate with CI

Easily integrate with CI to improve SDLC and catch regressions early.

### Introduce Developers to RL

Evals are essential for your AI application, from day one to millions of users.

We also believe RL is a superpower for AI applications. AI application
developers are already writing evals and performing "manual" RL through trial and
error or prompt engineering. EP should help developers structure evals in a way
that allows them to eventually use RL to gain an unfair advantage over their
competitors.

In short, be helpful in the short term with tooling for evals, and in the
long term with a standard for evals so you can build a proper data flywheel.