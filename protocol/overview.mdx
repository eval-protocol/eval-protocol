---
title: Protocol Overview
description: Technical specification of the Eval Protocol
---

# Protocol Overview

The Eval Protocol defines a standardized interface for creating, deploying, and evaluating evaluation functions across different platforms and environments.

## Core Principles

### Standardization
- **Consistent API**: Uniform interface across all evaluation functions
- **Portable Format**: Functions work across different platforms
- **Interoperability**: Seamless integration with existing tools

### Simplicity
- **Minimal Boilerplate**: Focus on evaluation logic, not infrastructure
- **Clear Semantics**: Intuitive function signatures and return values
- **Easy Testing**: Built-in support for local testing and validation

### Scalability
- **Async Support**: Handle high-throughput evaluation workloads
- **Batch Processing**: Efficient evaluation of large datasets
- **Distributed Deployment**: Scale across multiple machines and regions

## Protocol Specification

### Function Signature
All evaluation functions must follow this signature:

```python
@evaluation_function
def function_name(query: str, response: str, **kwargs) -> float:
    """
    Evaluate a model response against a query.
    
    Args:
        query: The input query or prompt
        response: The model's response to evaluate
        **kwargs: Additional context or configuration
        
    Returns:
        Score between 0.0 and 1.0, where 1.0 is perfect
    """
    pass
```

### Return Value Semantics
- **Range**: Must return a float between 0.0 and 1.0
- **Interpretation**: 1.0 represents perfect/optimal response
- **Consistency**: Same inputs should produce same outputs
- **Precision**: Use appropriate precision for your use case

### Metadata Requirements
Every evaluation function must include:

```python
@evaluation_function(
    name="function_name",
    description="Brief description of what this evaluates",
    version="1.0.0",
    tags=["category", "domain"],
    dependencies=["package1", "package2"]
)
def function_name(query: str, response: str) -> float:
    pass
```

## Data Types

### Core Types
```python
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

@dataclass
class EvaluationResult:
    score: float
    details: Optional[Dict[str, Any]] = None
    confidence: Optional[float] = None
    metadata: Optional[Dict[str, Any]] = None

@dataclass
class BatchEvaluationInput:
    queries: List[str]
    responses: List[str]
    context: Optional[Dict[str, Any]] = None
```

### Extended Types
For complex evaluation scenarios:

```python
@dataclass
class MultiModalInput:
    text: str
    images: Optional[List[str]] = None
    audio: Optional[List[str]] = None
    metadata: Optional[Dict[str, Any]] = None

@dataclass
class StructuredResponse:
    content: str
    format: str  # "json", "markdown", "code", etc.
    language: Optional[str] = None
```

## Configuration Schema

### Function Configuration
```yaml
function:
  name: "math_accuracy"
  version: "1.0.0"
  description: "Evaluates mathematical accuracy"
  
parameters:
  threshold: 0.9
  strict_mode: false
  
resources:
  cpu: "100m"
  memory: "128Mi"
  
deployment:
  min_replicas: 1
  max_replicas: 10
  target_latency: "100ms"
```

### Evaluation Configuration
```yaml
evaluation:
  functions:
    - name: "math_accuracy"
      weight: 0.4
    - name: "clarity_score"
      weight: 0.3
    - name: "completeness"
      weight: 0.3
  
  aggregation: "weighted_average"
  
  batch_size: 100
  timeout: 30
```

## Protocol Extensions

### Custom Metrics
Extend the protocol with domain-specific metrics:

```python
@evaluation_function.extend
class CustomMetric:
    def evaluate(self, query: str, response: str) -> Dict[str, float]:
        return {
            "primary_score": self.calculate_primary(query, response),
            "secondary_score": self.calculate_secondary(query, response),
            "confidence": self.calculate_confidence(query, response)
        }
```

### Streaming Evaluation
For real-time evaluation:

```python
@evaluation_function.streaming
async def streaming_evaluator(query: str, response_stream: AsyncIterator[str]) -> AsyncIterator[float]:
    async for chunk in response_stream:
        yield calculate_partial_score(query, chunk)
```

## Compliance & Testing

### Validation Requirements
- **Input validation**: Handle malformed inputs gracefully
- **Output validation**: Ensure scores are within valid range
- **Error handling**: Provide meaningful error messages
- **Performance**: Meet latency and throughput requirements

### Testing Framework
```python
from eval_protocol.testing import EvaluationFunctionTest

class TestMathAccuracy(EvaluationFunctionTest):
    function = math_accuracy
    
    def test_basic_cases(self):
        self.assert_score_equals("2+2=?", "4", 1.0)
        self.assert_score_equals("2+2=?", "5", 0.0)
    
    def test_edge_cases(self):
        self.assert_score_range("", "", 0.0, 1.0)
        self.assert_no_error("malformed input", "any response")
```

## Version Compatibility

### Semantic Versioning
- **Major**: Breaking changes to function signature
- **Minor**: New features, backward compatible
- **Patch**: Bug fixes, no API changes

### Migration Support
```python
@evaluation_function.migrate
def migrate_v1_to_v2(old_config: dict) -> dict:
    # Migration logic for configuration changes
    return new_config
```

## Next Steps

- **Implementation Guide**: Learn how to [create evaluation functions](/development/getting-started)
- **Data Types**: Explore [core data types](/protocol/core-data-types)
- **Deployment**: Understand [evaluation workflows](/protocol/evaluation-workflows)