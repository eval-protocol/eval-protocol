---
title: '@evaluation_function Decorator'
description: 'The core decorator for creating evaluation functions in Eval Protocol'
---

# @evaluation_function Decorator

The `@evaluation_function` decorator is the primary way to create evaluation functions in Eval Protocol. It transforms your evaluation logic into a standardized format that can be deployed across different platforms.

## Basic Usage

```python
from eval_protocol import evaluation_function
from eval_protocol.models import EvaluateResult, MetricResult
from typing import List, Dict, Any

@evaluation_function
def my_evaluation_function(
    messages: List[Dict[str, Any]],
    ground_truth: str = None,
    **kwargs: Any
) -> EvaluateResult:
    """Your evaluation logic here."""
    response = messages[-1].get("content", "")
    score = len(response.split()) / 100.0  # Simple word count scoring
    
    return EvaluateResult(
        score=min(score, 1.0),
        reason=f"Response has {len(response.split())} words",
        metrics={
            "word_count": MetricResult(
                score=score,
                success=len(response.split()) > 10,
                reason=f"Word count: {len(response.split())}"
            )
        }
    )
```

## Parameters

### Required Parameters

#### messages: List[Dict[str, Any]]
The conversation history including the model's response to evaluate. The structure follows the OpenAI chat format:

```python
[
    {"role": "user", "content": "What is AI?"},
    {"role": "assistant", "content": "AI is artificial intelligence..."}
]
```

### Optional Parameters

#### ground_truth: Any
The expected or reference answer for comparison. Can be a string, dict, or any serializable type:

```python
# String ground truth
ground_truth = "The answer is 42"

# Dict ground truth for structured data
ground_truth = {
    "answer": "42",
    "reasoning": "Based on the calculation..."
}
```

#### original_messages: List[Dict[str, Any]]
The original conversation before any modifications or preprocessing.

#### **kwargs: Any
Additional parameters that can be passed to your evaluation function:

```python
@evaluation_function
def flexible_evaluation(
    messages: List[Dict[str, Any]],
    max_length: int = 100,
    language: str = "en",
    **kwargs
) -> EvaluateResult:
    # Use custom parameters in your logic
    response = messages[-1].get("content", "")
    if len(response) > max_length:
        score = 0.5  # Penalize overly long responses
    # ... rest of logic
```

## Return Value

All evaluation functions must return an `EvaluateResult` object:

```python
from eval_protocol.models import EvaluateResult, MetricResult

return EvaluateResult(
    score=0.85,  # Primary score (0.0 to 1.0)
    reason="Response demonstrates good understanding",
    metrics={
        "accuracy": MetricResult(
            score=0.9,
            success=True,
            reason="Factually correct"
        ),
        "clarity": MetricResult(
            score=0.8,
            success=True,
            reason="Well-structured response"
        )
    }
)
```

## Advanced Features

### Multiple Metrics

Track multiple evaluation criteria:

```python
@evaluation_function
def comprehensive_evaluation(messages, **kwargs) -> EvaluateResult:
    response = messages[-1].get("content", "")
    
    # Evaluate different aspects
    word_count = len(response.split())
    has_citations = "http" in response or "[" in response
    is_helpful = any(word in response.lower() for word in ["because", "therefore", "due to"])
    
    metrics = {
        "length": MetricResult(
            score=min(word_count / 50, 1.0),
            success=10 <= word_count <= 200,
            reason=f"Response length: {word_count} words"
        ),
        "citations": MetricResult(
            score=1.0 if has_citations else 0.0,
            success=has_citations,
            reason="Contains citations" if has_citations else "No citations found"
        ),
        "helpfulness": MetricResult(
            score=1.0 if is_helpful else 0.0,
            success=is_helpful,
            reason="Contains explanatory language" if is_helpful else "Lacks explanation"
        )
    }
    
    # Calculate weighted overall score
    overall_score = (
        metrics["length"].score * 0.3 +
        metrics["citations"].score * 0.3 +
        metrics["helpfulness"].score * 0.4
    )
    
    return EvaluateResult(
        score=overall_score,
        reason=f"Comprehensive evaluation score: {overall_score:.2f}",
        metrics=metrics
    )
```

### Error Handling

Handle edge cases gracefully:

```python
@evaluation_function
def robust_evaluation(messages, ground_truth=None, **kwargs) -> EvaluateResult:
    try:
        if not messages:
            return EvaluateResult(
                score=0.0,
                reason="No messages provided",
                metrics={}
            )
        
        response = messages[-1].get("content", "")
        if not response:
            return EvaluateResult(
                score=0.0,
                reason="Empty response",
                metrics={}
            )
        
        # Your evaluation logic here
        score = evaluate_response(response, ground_truth)
        
        return EvaluateResult(
            score=score,
            reason=f"Evaluation completed successfully: {score}",
            metrics={"main_metric": MetricResult(score, score > 0.5, "Primary evaluation")}
        )
        
    except Exception as e:
        return EvaluateResult(
            score=0.0,
            reason=f"Evaluation error: {str(e)}",
            metrics={}
        )
```

## Best Practices

1. **Always validate inputs**: Check for empty or malformed messages
2. **Provide clear reasons**: Help users understand why they received a particular score
3. **Use meaningful metric names**: Make your evaluation criteria clear
4. **Handle edge cases**: Plan for empty responses, missing ground truth, etc.
5. **Keep it focused**: Each evaluation function should evaluate specific criteria
6. **Make it deterministic**: Same inputs should produce same outputs

## Testing Your Evaluation Function

```python
# Test with sample data
test_messages = [
    {"role": "user", "content": "Explain quantum computing"},
    {"role": "assistant", "content": "Quantum computing uses quantum mechanical phenomena..."}
]

result = my_evaluation_function(messages=test_messages)
print(f"Score: {result.score}")
print(f"Reason: {result.reason}")
print(f"Metrics: {result.metrics}")
```

## Integration with CLI

Once decorated, your function can be used with Eval Protocol CLI:

```bash
# Preview evaluation
reward-protocol preview --metrics-folders "my_metric=./path/to/function" --samples ./data.jsonl

# Deploy to production
reward-protocol deploy --id my-evaluator --metrics-folders "my_metric=./path/to/function"
```

## Next Steps

- Learn about [Data Models](/api-reference/data-models) for detailed type information
- Explore [Examples](/examples/overview) for real-world implementations
- Check the [CLI Reference](/cli-reference/overview) for deployment options