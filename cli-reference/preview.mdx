---
title: 'reward-protocol preview'
description: 'Quickly test evaluation functions with sample data before deployment'
---

# reward-protocol preview

The `preview` command allows you to quickly test evaluation functions with sample data, making it ideal for development, debugging, and validation before deploying to production.

## Basic Usage

```bash
reward-protocol preview --metrics-folders "metric_name=./path/to/metric" --samples ./samples.jsonl
```

## Command Syntax

```bash
reward-protocol preview [OPTIONS]
```

### Required Options

#### --metrics-folders
Specify evaluation functions to test using the format `name=path`.

```bash
# Single metric
reward-protocol preview --metrics-folders "word_count=./metrics/word_count"

# Multiple metrics
reward-protocol preview --metrics-folders "accuracy=./metrics/accuracy" "length=./metrics/length"
```

#### --samples
Path to JSONL file containing sample conversations to evaluate.

```bash
reward-protocol preview --samples ./test_data.jsonl
```

### Optional Parameters

#### --output, -o
Specify output file for results (default: prints to console).

```bash
reward-protocol preview \
  --metrics-folders "math=./rewards/math" \
  --samples ./samples.jsonl \
  --output ./preview_results.jsonl
```

#### --limit
Limit the number of samples to process.

```bash
reward-protocol preview \
  --metrics-folders "quality=./metrics/quality" \
  --samples ./large_dataset.jsonl \
  --limit 10
```

#### --parallel
Enable parallel processing for faster evaluation.

```bash
reward-protocol preview \
  --metrics-folders "speed_test=./metrics/speed" \
  --samples ./samples.jsonl \
  --parallel
```

#### --verbose, -v
Show detailed output and debugging information.

```bash
reward-protocol preview \
  --metrics-folders "debug=./metrics/debug" \
  --samples ./samples.jsonl \
  --verbose
```

#### --config
Use a configuration file for preview settings.

```bash
reward-protocol preview --config ./preview_config.yaml
```

## Sample Data Format

The samples file should be in JSONL format with each line containing a conversation:

### Basic Format

```json
{"messages": [{"role": "user", "content": "What is AI?"}, {"role": "assistant", "content": "AI stands for Artificial Intelligence..."}]}
{"messages": [{"role": "user", "content": "Explain quantum computing"}, {"role": "assistant", "content": "Quantum computing uses quantum mechanics..."}]}
```

### With Ground Truth

```json
{"messages": [{"role": "user", "content": "What is 2+2?"}, {"role": "assistant", "content": "2+2 equals 4"}], "ground_truth": "4"}
{"messages": [{"role": "user", "content": "Capital of France?"}, {"role": "assistant", "content": "Paris"}], "ground_truth": "Paris"}
```

### With Tool Calls

```json
{
  "messages": [
    {"role": "user", "content": "What's the weather in SF?"},
    {
      "role": "assistant",
      "content": "I'll check the weather for you.",
      "tool_calls": [
        {
          "id": "call_123",
          "type": "function",
          "function": {
            "name": "get_weather",
            "arguments": "{\"location\": \"San Francisco, CA\"}"
          }
        }
      ]
    }
  ],
  "ground_truth": {
    "tool_calls": [
      {
        "name": "get_weather",
        "arguments": {"location": "San Francisco, CA"}
      }
    ]
  }
}
```

### With Custom Parameters

```json
{
  "messages": [{"role": "assistant", "content": "Here's a Python function..."}],
  "ground_truth": "def factorial(n): return 1 if n <= 1 else n * factorial(n-1)",
  "language": "python",
  "max_length": 100
}
```

## Examples

### Basic Word Count Evaluation

```bash
# Create sample data
echo '{"messages": [{"role": "assistant", "content": "Short answer"}]}' > samples.jsonl
echo '{"messages": [{"role": "assistant", "content": "This is a much longer and more detailed response with multiple sentences and explanations."}]}' >> samples.jsonl

# Run preview
reward-protocol preview \
  --metrics-folders "word_count=examples/metrics/word_count" \
  --samples samples.jsonl
```

**Output:**
```
Sample 1:
  word_count: 0.20 (2 words - Response is quite short)

Sample 2:
  word_count: 0.80 (16 words - Good response length)

Average Scores:
  word_count: 0.50
```

### Math Problem Evaluation

```bash
# Create math samples
cat > math_samples.jsonl << EOF
{"messages": [{"role": "user", "content": "What is 15 + 27?"}, {"role": "assistant", "content": "15 + 27 = 42"}], "ground_truth": "42"}
{"messages": [{"role": "user", "content": "What is 8 × 6?"}, {"role": "assistant", "content": "8 times 6 equals 48"}], "ground_truth": "48"}
{"messages": [{"role": "user", "content": "What is 100 ÷ 4?"}, {"role": "assistant", "content": "100 divided by 4 is 25"}], "ground_truth": "25"}
EOF

# Preview with math evaluator
reward-protocol preview \
  --metrics-folders "math_accuracy=./rewards/math" \
  --samples math_samples.jsonl \
  --verbose
```

### Multiple Metrics Evaluation

```bash
reward-protocol preview \
  --metrics-folders \
    "accuracy=./metrics/accuracy" \
    "length=./metrics/length" \
    "clarity=./metrics/clarity" \
  --samples comprehensive_samples.jsonl \
  --output detailed_results.jsonl
```

### Tool Calling Evaluation

```bash
# Create tool call samples
cat > tool_samples.jsonl << EOF
{
  "messages": [
    {"role": "user", "content": "Get weather for NYC"},
    {
      "role": "assistant", 
      "tool_calls": [
        {
          "id": "call_1",
          "type": "function",
          "function": {
            "name": "get_weather",
            "arguments": "{\"location\": \"New York, NY\"}"
          }
        }
      ]
    }
  ],
  "ground_truth": {
    "tool_calls": [{"name": "get_weather", "arguments": {"location": "New York, NY"}}]
  }
}
EOF

reward-protocol preview \
  --metrics-folders "tool_calling=./rewards/function_calling" \
  --samples tool_samples.jsonl
```

### Limited Sample Testing

```bash
# Test first 5 samples only
reward-protocol preview \
  --metrics-folders "quality=./metrics/quality" \
  --samples large_dataset.jsonl \
  --limit 5 \
  --parallel
```

## Configuration File

For complex preview setups, use a configuration file:

### preview_config.yaml

```yaml
metrics_folders:
  accuracy: "./rewards/accuracy"
  reasoning: "./rewards/reasoning" 
  format: "./rewards/format"

samples_file: "./test_samples.jsonl"

settings:
  limit: 50
  parallel: true
  verbose: true
  output: "./preview_results.jsonl"

# Custom parameters passed to evaluation functions
custom_params:
  max_response_length: 500
  evaluation_mode: "strict"
  language: "en"
```

```bash
reward-protocol preview --config preview_config.yaml
```

## Output Formats

### Console Output (Default)

```
Processing 3 samples...

Sample 1 (ID: sample_001):
  accuracy: 0.95 ✓ (Exact match with ground truth)
  length: 0.60 ✓ (Appropriate response length)
  clarity: 0.85 ✓ (Clear and well-structured)
  Overall: 0.80

Sample 2 (ID: sample_002):
  accuracy: 0.70 ⚠ (Partially correct)
  length: 0.90 ✓ (Good response length)
  clarity: 0.75 ✓ (Mostly clear)
  Overall: 0.78

Sample 3 (ID: sample_003):
  accuracy: 0.40 ✗ (Incorrect answer)
  length: 0.30 ⚠ (Too short)
  clarity: 0.60 ⚠ (Somewhat unclear)
  Overall: 0.43

Summary:
  Total samples: 3
  Average scores:
    accuracy: 0.68
    length: 0.60
    clarity: 0.73
    overall: 0.67

  Success rates:
    accuracy: 66.7% (2/3 passed)
    length: 66.7% (2/3 passed)
    clarity: 100.0% (3/3 passed)
```

### JSON Output (--output)

```json
{
  "summary": {
    "total_samples": 3,
    "average_scores": {
      "accuracy": 0.68,
      "length": 0.60,
      "clarity": 0.73,
      "overall": 0.67
    },
    "success_rates": {
      "accuracy": 0.667,
      "length": 0.667,
      "clarity": 1.0
    }
  },
  "samples": [
    {
      "id": "sample_001",
      "messages": [...],
      "ground_truth": "expected answer",
      "results": {
        "accuracy": {
          "score": 0.95,
          "success": true,
          "reason": "Exact match with ground truth",
          "metrics": {...}
        }
      },
      "overall_score": 0.80
    }
  ]
}
```

## Error Handling

### Common Issues

#### Missing Metric Folder

```bash
Error: Metric folder not found: ./nonexistent/path
```

**Solution:**
```bash
# Check path exists
ls -la ./metrics/
# Use correct path
reward-protocol preview --metrics-folders "metric=./correct/path"
```

#### Invalid Sample Format

```bash
Error: Invalid JSON in samples file at line 3
```

**Solution:**
```bash
# Validate JSON format
cat samples.jsonl | jq .  # Check each line
# Fix JSON syntax errors
```

#### Missing Evaluation Function

```bash
Error: Evaluation function 'my_function' not found in ./metrics/my_metric
```

**Solution:**
```bash
# Check function exists
ls -la ./metrics/my_metric/
# Ensure __init__.py and function are present
```

#### Import Errors

```bash
Error: Failed to import evaluation function: ModuleNotFoundError
```

**Solution:**
```bash
# Check Python path
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
# Install missing dependencies
pip install required-package
```

## Performance Tips

### Large Datasets

```bash
# Process in chunks
reward-protocol preview \
  --samples large_file.jsonl \
  --limit 100 \
  --parallel \
  --metrics-folders "fast_metric=./metrics/optimized"
```

### Memory Optimization

```bash
# Use streaming for large files
reward-protocol preview \
  --samples huge_dataset.jsonl \
  --limit 50 \
  --verbose \
  --metrics-folders "lightweight=./metrics/simple"
```

### Development Workflow

```bash
# Quick iteration cycle
# 1. Test with small sample
reward-protocol preview \
  --samples debug_samples.jsonl \
  --limit 3 \
  --verbose \
  --metrics-folders "dev=./metrics/development"

# 2. Expand to larger test set
reward-protocol preview \
  --samples test_samples.jsonl \
  --limit 20 \
  --metrics-folders "dev=./metrics/development"

# 3. Full validation
reward-protocol preview \
  --samples validation_samples.jsonl \
  --parallel \
  --metrics-folders "final=./metrics/production"
```

## Integration with Other Commands

### Generate Samples from Run Output

```bash
# Run evaluation to generate sample data
reward-protocol run --config-name eval --config-path conf/

# Preview using generated samples
reward-protocol preview \
  --samples outputs/2024-01-15_14-30-45/preview_input_output_pairs.jsonl \
  --metrics-folders "test=./metrics/test"
```

### Validate Before Deploy

```bash
# Preview with production metrics
reward-protocol preview \
  --samples validation_set.jsonl \
  --metrics-folders "prod=./metrics/production" \
  --output validation_results.jsonl

# If results look good, deploy
reward-protocol deploy \
  --id production-evaluator \
  --metrics-folders "prod=./metrics/production" \
  --force
```

## Best Practices

1. **Start Small**: Use `--limit` for initial testing
2. **Use Verbose Mode**: Enable `--verbose` for debugging
3. **Test Edge Cases**: Include edge cases in sample data
4. **Validate Format**: Check sample JSON format before running
5. **Compare Metrics**: Test multiple metrics together
6. **Save Results**: Use `--output` for important test runs
7. **Iterate Quickly**: Use preview for rapid development cycles

## Next Steps

- Learn about [Run](/cli-reference/run) for comprehensive evaluations
- Explore [Deploy](/cli-reference/deploy) for production deployment
- Check [Examples](/examples/overview) for sample data patterns