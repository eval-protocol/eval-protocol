---
title: Introduction to Eval Protocol (EP)
sidebarTitle: Introduction
---

import { Workflow } from "/snippets/workflow.jsx"

EP is an [open specification](/specification), [Python
SDK](https://github.com/eval-protocol/python-sdk), and [pytest
plugin](https://docs.pytest.org/en/stable/how-to/writing_plugins.html) built to
standardize how developers author evals for large language model (LLM)
applications. It ensures that writing evals, storing traces, and saving eval
results follow a consistent pattern, and can be reused across various tasks like
model selection, prompt/context engineering, CI/CD, and fine-tuning for
real-world use cases‚Äîfrom tricky markdown generation and data extraction tasks
to complex customer service agents.

<Workflow />

For example, here is an evaluation function that checks if model responses contain the required number of highlighted sections (like **bold** or *italic* text):

```python
def markdown_format_evaluate(messages: List[Message], ground_truth: Optional[str]=None, **kwargs) -> EvaluateResult:
    """Check if response contains required number of highlighted sections."""
    assistant_response = messages[-1].content
    
    if not assistant_response:
        return EvaluateResult(score=0.0, reason="‚ùå No assistant response found")
    
    required_highlights = int(ground_truth)
    
    # Count highlighted sections using regex
    highlights = re.findall(r"\*[^\n\*]*\*", assistant_response)
    double_highlights = re.findall(r"\*\*[^\n\*]*\*\*", assistant_response)
    
    actual_count = 0
    for highlight in highlights:
        if highlight.strip("*").strip():
            actual_count += 1
    for highlight in double_highlights:
        if highlight.removeprefix("**").removesuffix("**").strip():
            actual_count += 1
    
    meets_requirement = actual_count >= required_highlights
    
    return EvaluateResult(
        score=1.0 if meets_requirement else 0.0,
        reason=f"{'‚úÖ' if meets_requirement else '‚ùå'} Found {actual_count} highlights (required: {required_highlights})"
    )
```

For a complete step-by-step tutorial with detailed explanations, dataset examples, and configuration options, see our [Single-turn Evals](/single-turn-evals) guide.

For a more advanced example that includes MCP and user simulation, check out our
implementation of [ùúè¬≤-bench](#), a benchmark for evaluating conversational
agents in a dual control environment.


### Next Steps

 - [Specification](/specification)
 - [Why Eval Protocol?](/why)
 - [Principles](/principles)
 - [Single-turn Evals](/quickstart)
