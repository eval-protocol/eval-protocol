---
title: Introduction to Eval Protocol (EP)
sidebarTitle: Introduction
---

import { Workflow } from "/snippets/workflow.jsx"

EP is an [open specification](/specification), [Python
SDK](https://github.com/eval-protocol/python-sdk), and [pytest
plugin](https://docs.pytest.org/en/stable/how-to/writing_plugins.html) that
provides a standardized way to write evaluations for large language model (LLM)
applications. Start with simple single-turn evals for model selection and prompt
engineering, then scale up to complex multi-turn reinforcement learning (RL) for
agents using Model Context Protocol (MCP). EP ensures consistent patterns for
writing evals, storing traces, and saving results‚Äîenabling you to build
sophisticated agent evaluations that work across real-world scenarios, from
markdown generation tasks to customer service agents with tool calling
capabilities.

<Workflow />

For example, here is a standard eval function for a single-turn eval that checks
if model responses contain the required number of highlighted sections (like
**bold** or *italic* text):

```python test_markdown_format.py
def markdown_format_evaluate(messages: List[Message], ground_truth: Optional[str]=None, **kwargs) -> EvaluateResult:
    """Check if response contains required number of highlighted sections."""
    assistant_response = messages[-1].content
    
    if not assistant_response:
        return EvaluateResult(score=0.0, reason="‚ùå No assistant response found")
    
    required_highlights = int(ground_truth)
    
    # Count highlighted sections using regex
    highlights = re.findall(r"\*[^\n\*]*\*", assistant_response)
    double_highlights = re.findall(r"\*\*[^\n\*]*\*\*", assistant_response)
    
    actual_count = 0
    for highlight in highlights:
        if highlight.strip("*").strip():
            actual_count += 1
    for highlight in double_highlights:
        if highlight.removeprefix("**").removesuffix("**").strip():
            actual_count += 1
    
    meets_requirement = actual_count >= required_highlights
    
    return EvaluateResult(
        score=1.0 if meets_requirement else 0.0,
        reason=f"{'‚úÖ' if meets_requirement else '‚ùå'} Found {actual_count} highlights (required: {required_highlights})"
    )
```

For a complete step-by-step tutorial with detailed explanations, dataset
examples, and configuration options, see our [Single-turn
Evals](/single-turn-evals) guide.

For a more advanced example that includes MCP and user simulation, check out our
implementation of [ùúè¬≤-bench](#), a benchmark for evaluating conversational
agents in a dual control environment.


### Next Steps

 - [Specification](/specification)
 - [Why Eval Protocol?](/why)
 - [Principles](/principles)
 - [Single-turn Evals](/quickstart)
