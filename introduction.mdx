---
title: Introduction to Eval Protocol (EP)
sidebarTitle: Introduction
---

EP is an [open specification](/specification), [Python
SDK](https://github.com/eval-protocol/python-sdk), pytest wrapper, and suite of
tools that provides a standardized way to write evaluations for large language
model (LLM) applications. Start with simple single-turn evals for model
selection and prompt engineering, then scale up to complex multi-turn
reinforcement learning (RL) for agents using Model Context Protocol (MCP). EP
ensures consistent patterns for writing evals, storing traces, and saving
results‚Äîenabling you to build sophisticated agent evaluations that work across
real-world scenarios, from markdown generation tasks to customer service agents
with tool calling capabilities.

<Frame caption={<span><a href="/tutorial/reviewing-rollouts-ui">Log Viewer: Monitor your evaluation rollouts in real time.</a></span>}>
  <img src="./assets/ui.png" />
</Frame>


## Getting Started

Ready to dive in? Install EP with a single command and start evaluating your models:

```shell
pip install eval-protocol
```

> **Tracking Issue:** Versioneer Setup Requirements. Make sure to setup `versioneer` in your `setup.cfg` like so:
>
> ```cfg setup.cfg
> [versioneer]
> VCS = git
> style = pep440
> versionfile_source = evalprotocol/_version.py
> versionfile_build = evalprotocol/_version.py
> tag_prefix = v
> parentdir_prefix = evalprotocol-
> ```

## Quick Example

Here's a simple test function that checks if a model's response contains **bold** text formatting:

```python test_bold_format.py
from eval_protocol.models import EvaluateResult, EvaluationRow, Message
from eval_protocol.pytest import default_single_turn_rollout_processor, evaluation_test

@evaluation_test(
    input_messages=[
        [
            Message(role="system", content="You are a helpful assistant. Use bold text to highlight important information."),
            Message(role="user", content="Explain why **evaluations** matter for building AI agents. Make it dramatic!"),
        ],
    ],
    model=["accounts/fireworks/models/llama-v3p1-8b-instruct"],
    rollout_processor=default_single_turn_rollout_processor,
    mode="pointwise",
)
def test_bold_format(row: EvaluationRow) -> EvaluationRow:
    """
    Simple evaluation that checks if the model's response contains bold text.
    """
    
    assistant_response = row.messages[-1].content
    
    # Check if response contains **bold** text
    has_bold = "**" in assistant_response
    
    if has_bold:
        result = EvaluateResult(score=1.0, reason="‚úÖ Response contains bold text")
    else:
        result = EvaluateResult(score=0.0, reason="‚ùå No bold text found")
    
    row.evaluation_result = result
    return row
```

Run the test:

```shell
pytest test_bold_format.py
```

The test will run and output the results to the console:

```shell
(eval-protocol) ‚ûú  evalprotocol git:(main) ‚úó pytest test_bold_format.py
===================================================== test session starts ======================================================
platform darwin -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0
rootdir: /Users/YOUR_PATH/evalprotocol
configfile: pyproject.toml
plugins: anyio-4.9.0, hydra-core-1.3.2
collected 1 item                                                                                                               

test_bold_format.py .                                                                                                    [100%]

======================================================= warnings summary =======================================================
../../python-sdk/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py:323
../../python-sdk/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py:323
  /Users/YOUR_PATH/python-sdk/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
    warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)

test_bold_format.py::test_bold_format[accounts/fireworks/models/gpt-oss-120b-input_messages0]
  /Users/YOUR_PATH/python-sdk/eval_protocol/pytest/utils.py:26: DeprecationWarning: There is no current event loop
    loop = asyncio.get_event_loop()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
================================================ 1 passed, 3 warnings in 7.98s =================================================
```




## Learn More

For a complete step-by-step tutorial of a slightly more complex example with
detailed explanations, dataset examples, and configuration options, see our
[Single-turn eval](/tutorial/single-turn-eval-static) tutorial.

For a more advanced example that includes MCP and user simulation, check out our
implementation of [ùúè¬≤-bench](/tutorial/multi-turn-eval-user-simulation), a benchmark for evaluating conversational
agents in a dual control environment.

### Next Steps

 - [Specification](/specification)
 - [Why Eval Protocol?](/why)
 - [Principles](/principles)
 - [Single-turn eval](/tutorial/single-turn-eval-static)
 - [Multi-turn eval](/tutorial/multi-turn-eval-user-simulation)
