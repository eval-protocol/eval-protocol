---
title: Development Getting Started
description: Advanced guide for developing with Eval Protocol
---

# Development Getting Started

This guide covers advanced development patterns and best practices for building robust reward functions and integrating them into production systems.

## Development Environment Setup

### Prerequisites
- Python 3.8+ with virtual environment support
- Git for version control
- Docker for containerized development (optional)
- IDE with Python support (VS Code, PyCharm, etc.)

### Local Development
```bash
# Clone the development template
git clone https://github.com/fireworks-ai/reward-protocol-template.git
cd reward-protocol-template

# Set up environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install development dependencies
pip install -e ".[dev]"

# Install pre-commit hooks
pre-commit install
```

### Docker Development
```bash
# Use the development container
docker run -it --rm \
  -v $(pwd):/workspace \
  -w /workspace \
  fireworks/reward-protocol:dev

# Or use docker-compose
docker-compose up -d dev
```

## Advanced Function Development

### Complex Evaluation Logic
```python
from eval_protocol import reward_function
from eval_protocol.types import EvaluationResult
import asyncio
from typing import Dict, Any

@reward_function
async def advanced_evaluator(
    query: str, 
    response: str, 
    context: Dict[str, Any] = None
) -> EvaluationResult:
    """
    Advanced evaluator with detailed results and context.
    """
    # Parallel evaluation of multiple criteria
    tasks = [
        evaluate_accuracy(query, response),
        evaluate_relevance(query, response),
        evaluate_clarity(response),
        evaluate_completeness(query, response, context)
    ]
    
    accuracy, relevance, clarity, completeness = await asyncio.gather(*tasks)
    
    # Weighted scoring with context
    weights = context.get('weights', {
        'accuracy': 0.4,
        'relevance': 0.3,
        'clarity': 0.2,
        'completeness': 0.1
    })
    
    score = (
        accuracy * weights['accuracy'] +
        relevance * weights['relevance'] +
        clarity * weights['clarity'] +
        completeness * weights['completeness']
    )
    
    return EvaluationResult(
        score=score,
        details={
            'accuracy': accuracy,
            'relevance': relevance,
            'clarity': clarity,
            'completeness': completeness
        },
        confidence=calculate_confidence(accuracy, relevance, clarity),
        metadata={
            'weights_used': weights,
            'evaluation_time': time.time(),
            'version': "2.0.0"
        }
    )
```

### Multi-Modal Support
```python
from eval_protocol.types import MultiModalInput

@reward_function
def multimodal_evaluator(
    query: str,
    response: str,
    images: List[str] = None,
    audio: List[str] = None
) -> float:
    """
    Evaluate responses that include text, images, and audio.
    """
    score = 0.0
    
    # Text evaluation
    text_score = evaluate_text_response(query, response)
    score += text_score * 0.6
    
    # Image evaluation
    if images:
        image_score = evaluate_image_relevance(query, images)
        score += image_score * 0.3
    
    # Audio evaluation
    if audio:
        audio_score = evaluate_audio_quality(audio)
        score += audio_score * 0.1
    
    return min(score, 1.0)
```

## Performance Optimization

### Caching Strategies
```python
from functools import lru_cache
from eval_protocol.cache import RewardCache

class OptimizedEvaluator:
    def __init__(self):
        self.cache = RewardCache(max_size=10000)
    
    @reward_function
    def cached_evaluator(self, query: str, response: str) -> float:
        # Check cache first
        cache_key = f"{hash(query)}_{hash(response)}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        # Compute score
        score = self._compute_score(query, response)
        
        # Store in cache
        self.cache[cache_key] = score
        return score
    
    @lru_cache(maxsize=1000)
    def _compute_expensive_feature(self, text: str) -> float:
        # Expensive computation that benefits from caching
        return complex_nlp_analysis(text)
```

### Batch Processing
```python
from eval_protocol.batch import BatchProcessor

@reward_function.batch
def batch_evaluator(queries: List[str], responses: List[str]) -> List[float]:
    """
    Efficient batch evaluation with vectorized operations.
    """
    # Vectorized preprocessing
    query_embeddings = embed_texts(queries)
    response_embeddings = embed_texts(responses)
    
    # Batch similarity computation
    similarities = compute_similarity_matrix(query_embeddings, response_embeddings)
    
    # Convert to scores
    scores = [similarity_to_score(sim) for sim in similarities]
    
    return scores
```

## Error Handling & Monitoring

### Robust Error Handling
```python
import logging
from eval_protocol.exceptions import RewardProtocolError

logger = logging.getLogger(__name__)

@reward_function
def robust_evaluator(query: str, response: str) -> float:
    """
    Evaluator with comprehensive error handling.
    """
    try:
        # Validate inputs
        if not query.strip():
            logger.warning("Empty query provided")
            return 0.0
        
        if not response.strip():
            logger.warning("Empty response provided")
            return 0.0
        
        # Main evaluation logic
        score = compute_score(query, response)
        
        # Validate output
        if not 0.0 <= score <= 1.0:
            logger.error(f"Score {score} out of valid range [0, 1]")
            raise RewardProtocolError(f"Invalid score: {score}")
        
        return score
        
    except Exception as e:
        logger.error(f"Evaluation failed: {str(e)}")
        # Return default score instead of crashing
        return 0.0
```

### Performance Monitoring
```python
import time
from eval_protocol.monitoring import MetricsCollector

metrics = MetricsCollector()

@reward_function
def monitored_evaluator(query: str, response: str) -> float:
    """
    Evaluator with built-in monitoring.
    """
    start_time = time.time()
    
    try:
        score = compute_score(query, response)
        
        # Record success metrics
        metrics.record_evaluation_success(
            function_name="monitored_evaluator",
            latency=time.time() - start_time,
            score=score
        )
        
        return score
        
    except Exception as e:
        # Record failure metrics
        metrics.record_evaluation_failure(
            function_name="monitored_evaluator",
            error=str(e),
            latency=time.time() - start_time
        )
        raise
```

## Testing & Validation

### Comprehensive Testing
```python
import pytest
from eval_protocol.testing import RewardFunctionTest

class TestAdvancedEvaluator(RewardFunctionTest):
    function = advanced_evaluator
    
    def test_score_range(self):
        """Test that scores are always in valid range."""
        test_cases = [
            ("What is 2+2?", "4"),
            ("", ""),
            ("Complex query", "Complex response"),
            ("Query with unicode ðŸš€", "Response with unicode âœ¨")
        ]
        
        for query, response in test_cases:
            score = self.function(query, response)
            assert 0.0 <= score <= 1.0, f"Score {score} out of range for {query}, {response}"
    
    def test_consistency(self):
        """Test that same inputs produce same outputs."""
        query, response = "Test query", "Test response"
        
        scores = [self.function(query, response) for _ in range(10)]
        assert all(s == scores[0] for s in scores), "Inconsistent scores"
    
    def test_edge_cases(self):
        """Test edge cases and error conditions."""
        edge_cases = [
            ("", ""),
            ("Query", ""),
            ("", "Response"),
            ("A" * 10000, "B" * 10000),  # Very long inputs
            ("Special chars: @#$%^&*()", "More special chars: <>?[]{}"),
            ("Unicode: ä½ å¥½ä¸–ç•Œ", "Response: Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…")
        ]
        
        for query, response in edge_cases:
            try:
                score = self.function(query, response)
                assert 0.0 <= score <= 1.0
            except Exception as e:
                pytest.fail(f"Function failed on edge case {query}, {response}: {e}")
    
    @pytest.mark.asyncio
    async def test_async_behavior(self):
        """Test async function behavior."""
        if asyncio.iscoroutinefunction(self.function):
            score = await self.function("Test", "Response")
            assert 0.0 <= score <= 1.0
```

### Load Testing
```python
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor

async def load_test_evaluator(function, num_requests=1000):
    """
    Test function performance under load.
    """
    async def single_request():
        return function("Test query", "Test response")
    
    start_time = time.time()
    
    # Create tasks
    tasks = [single_request() for _ in range(num_requests)]
    
    # Run in parallel
    results = await asyncio.gather(*tasks)
    
    end_time = time.time()
    
    # Calculate metrics
    total_time = end_time - start_time
    requests_per_second = num_requests / total_time
    
    print(f"Processed {num_requests} requests in {total_time:.2f}s")
    print(f"Throughput: {requests_per_second:.2f} requests/second")
    
    return results
```

## Deployment Patterns

### Containerized Deployment
```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy function code
COPY reward_functions/ ./reward_functions/

# Install functions
RUN pip install -e .

# Health check
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
  CMD python -c "from reward_functions import health_check; health_check()"

# Run server
CMD ["reward-protocol", "serve", "--host", "0.0.0.0", "--port", "8000"]
```

### Kubernetes Deployment
```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: reward-function-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: reward-function
  template:
    metadata:
      labels:
        app: reward-function
    spec:
      containers:
      - name: reward-function
        image: your-registry/reward-function:latest
        ports:
        - containerPort: 8000
        env:
        - name: EVAL_PROTOCOL_LOG_LEVEL
          value: "INFO"
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
```

## Next Steps

- **Best Practices**: Learn [development best practices](/development/best-practices)
- **Deployment**: Explore [deployment strategies](/development/deployment)
- **Examples**: Study [advanced examples](/examples/overview)
- **API Reference**: Check the [complete API documentation](/api-reference/overview)