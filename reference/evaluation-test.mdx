---
title: "@evaluation_test"
description: "Create pytest-based evaluation tests for AI model evaluation with support for batch, pointwise, and groupwise evaluation modes"
---

The [`@evaluation_test`](https://github.com/eval-protocol/python-sdk/blob/main/eval_protocol/pytest/evaluation_test.py) decorator is the core component for creating pytest-based evaluation tests in the Evaluation Protocol. It enables you to evaluate AI models by running rollouts and applying evaluation criteria to measure performance.

## Key Concepts

Before diving into the API, it's important to understand the terminology used in the Evaluation Protocol:

- **Invocation**: A single execution of a test function that can generate 1 or more experiments
- **Experiment**: A group of runs for a combination of parameters (multiple experiments if `num_runs > 1`)
- **Run**: A group of rollouts (multiple run IDs if `num_runs > 1`)
- **Rollout**: The execution/process that produces a trajectory
- **Trajectory**: The result produced by a rollout — a list of OpenAI Chat Completion messages
- **Row**: Both input and output of an evaluation (e.g., a task within a dataset)
- **Dataset**: A collection of rows (List[EvaluationRow])
- **Eval**: A rubric implemented in the test function body that produces a score from 0 to 1

Each of these entities has a unique ID for easy grouping and identification.

## Basic Usage

```python
from typing import List
from eval_protocol.pytest import evaluation_test
from eval_protocol.models import EvaluationRow

@evaluation_test(
    completion_params=[
        {"model": "gpt-4", "temperature": 0.1},
        {"model": "gpt-3.5-turbo", "temperature": 0.1},
    ],
    input_dataset=["path/to/dataset.jsonl"],
    passed_threshold=0.8,
    mode="all"
)
def test_math_reasoning(rows: List[EvaluationRow]) -> List[EvaluationRow]:
    """Evaluate mathematical reasoning capabilities."""
    for row in rows:
        # Your evaluation logic here
        score = evaluate_math_reasoning(row.messages)
        row.evaluation_result.score = score
    
    return rows
```

## Parameters

Only `completion_params` is required; all other parameters are optional.

<ParamField path="completion_params" type="List[dict]" required>
Generation parameters for the rollout. Must include a `model` field.
</ParamField>

<ParamField path="input_messages" type="Optional[List[InputMessagesParam]]">
Messages to send to the model. Useful when you don't have a dataset but can hard-code messages. Will be passed as "input_dataset" to the test function.
</ParamField>

<ParamField path="input_dataset" type="Optional[List[DatasetPathParam]]">
Paths to JSONL datasets. Provide a `dataset_adapter` to convert custom dataset formats to EvaluationRows.
</ParamField>

<ParamField path="dataset_adapter" type="Callable[[List[Dict[str, Any]]], Dataset]">
Function to convert input dataset to a list of EvaluationRows. Defaults to `default_dataset_adapter`.
</ParamField>

<ParamField path="rollout_processor" type="RolloutProcessor">
Function used to perform the rollout. Defaults to `NoOpRolloutProcessor()`.
</ParamField>

<ParamField path="evaluation_test_kwargs" type="Optional[List[EvaluationInputParam]]">
Additional keyword arguments for the evaluation function.
</ParamField>

<ParamField path="rollout_processor_kwargs" type="Optional[RolloutProcessorInputParam]">
Additional keyword arguments for the rollout processor.
</ParamField>

<ParamField path="aggregation_method" type="AggregationMethod">
How to aggregate scores across runs. One of: "mean", "max", "min". Defaults to "mean".
</ParamField>

<ParamField path="passed_threshold" type="Optional[Union[EvaluationThreshold, float, dict]]">
Threshold configuration for test success. Can be a float or EvaluationThreshold object. Success rate must be above `success`, and if set, standard error must be below `standard_error`.
</ParamField>

<ParamField path="num_runs" type="int">
Number of times to repeat the rollout and evaluations. Defaults to 1.
</ParamField>

<ParamField path="max_dataset_rows" type="Optional[int]">
Limit dataset to the first N rows.
</ParamField>

<ParamField path="mcp_config_path" type="Optional[str]">
Path to MCP config file that follows MCPMultiClientConfiguration schema.
</ParamField>

<ParamField path="max_concurrent_rollouts" type="int">
Maximum number of concurrent rollouts to run in parallel. Defaults to 8.
</ParamField>

<ParamField path="max_concurrent_evaluations" type="int">
Maximum number of concurrent evaluations to run in parallel. Defaults to 64.
</ParamField>

<ParamField path="server_script_path" type="Optional[str]">
Path to the MCP server script to run. Defaults to "examples/tau2_mcp/server.py".
</ParamField>

<ParamField path="steps" type="int">
Number of rollout steps to execute. Defaults to 30.
</ParamField>

<ParamField path="mode" type="EvaluationTestMode">
Evaluation mode. "pointwise" (default) applies test function to each row individually. "groupwise" applies test function to a group of rollout results from the same original row (for use cases such as DPO/GRPO). "all" applies test function to the whole dataset.
</ParamField>

<ParamField path="combine_datasets" type="bool">
Whether to combine multiple datasets. Defaults to True.
</ParamField>

<ParamField path="logger" type="Optional[DatasetLogger]">
DatasetLogger to use for logging. If not provided, a default logger will be used.
</ParamField>

<ParamField path="exception_handler_config" type="Optional[ExceptionHandlerConfig]">
Configuration for exception handling and backoff retry logic. If not provided, a default configuration will be used with common retryable exceptions. See the ExceptionHandlerConfig section below for detailed configuration options.
</ParamField>

## ExceptionHandlerConfig

The `ExceptionHandlerConfig` parameter allows you to customize exception handling and retry logic for your evaluation tests. This configuration is defined in [`eval_protocol.pytest.exception_config.ExceptionHandlerConfig`](https://github.com/eval-protocol/python-sdk/blob/main/eval_protocol/pytest/exception_config.py).

### Key Features

- **Retryable Exceptions**: Configure which exceptions should trigger retry attempts
- **Backoff Strategies**: Choose between exponential or constant backoff with configurable delays
- **Environment Variable Overrides**: Automatically respect `EP_MAX_RETRY` and `EP_FAIL_ON_MAX_RETRY` settings
- **Custom Giveup Logic**: Define custom conditions for when to stop retrying

### Configuration Classes

#### ExceptionHandlerConfig

The main configuration class that controls exception handling behavior:

```python
@dataclass
class ExceptionHandlerConfig:
    # Exceptions that should be retried using backoff
    retryable_exceptions: Set[Type[Exception]] = DEFAULT_RETRYABLE_EXCEPTIONS
    
    # Backoff configuration
    backoff_config: BackoffConfig = BackoffConfig()
```

#### BackoffConfig

Controls the retry backoff behavior:

```python
@dataclass
class BackoffConfig:
    strategy: str = "expo"           # "expo" or "constant"
    base_delay: float = 1.0         # Base delay in seconds
    max_delay: float = 60.0         # Maximum delay in seconds
    max_tries: int = 3              # Maximum number of retry attempts
    jitter: Union[None, Callable] = None  # Jitter function for randomization
    factor: float = 2.0             # Factor for exponential backoff
    raise_on_giveup: bool = True    # Whether to raise exception when giving up
    giveup_func: Callable[[Exception], bool] = lambda e: False  # Custom giveup logic
```

### Default Configuration

By default, the following exceptions are considered retryable:
- **Standard library exceptions**: `ConnectionError`, `TimeoutError`, `OSError`
- **Requests library exceptions**: `requests.exceptions.ConnectionError`, `requests.exceptions.Timeout`, `requests.exceptions.HTTPError`, `requests.exceptions.RequestException`
- **HTTPX library exceptions**: `httpx.ConnectError`, `httpx.TimeoutException`, `httpx.NetworkError`, `httpx.RemoteProtocolError`

### Backoff Strategies

#### Exponential Backoff (Default)
- Starts with `base_delay` and multiplies by `factor` each retry
- Good for transient failures that may resolve quickly
- Example: 1s → 2s → 4s → 8s → 16s (capped at `max_delay`)

#### Constant Backoff
- Uses the same delay (`base_delay`) for all retries
- Good for predictable, consistent retry timing
- Example: 2s → 2s → 2s → 2s

### Environment Variable Integration

The configuration automatically respects these environment variables:

- `EP_MAX_RETRY`: Overrides `max_tries` in BackoffConfig
- `EP_FAIL_ON_MAX_RETRY`: Controls `raise_on_giveup` behavior

### Example Usage

#### Basic Custom Configuration

```python
from eval_protocol.pytest.exception_config import ExceptionHandlerConfig, BackoffConfig

# Custom exception handling configuration
custom_config = ExceptionHandlerConfig(
    backoff_config=BackoffConfig(
        strategy="expo",
        base_delay=2.0,
        max_delay=120.0,
        max_tries=5,
        jitter=None
    )
)

@evaluation_test(
    completion_params=[{"model": "gpt-4"}],
    exception_handler_config=custom_config
)
def test_with_custom_retry_logic(rows: List[EvaluationRow]) -> List[EvaluationRow]:
    # Your evaluation logic here
    return rows
```

#### Aggressive Retry Strategy

```python
# Aggressive retry for unreliable networks
aggressive_config = ExceptionHandlerConfig(
    backoff_config=BackoffConfig(
        strategy="expo",
        base_delay=0.5,    # Start with 0.5s delay
        max_delay=30.0,    # Cap at 30s
        max_tries=10,      # Try up to 10 times
        jitter=None        # No jitter for predictable timing
    )
)
```

#### Conservative Retry Strategy

```python
# Conservative retry for stable networks
conservative_config = ExceptionHandlerConfig(
    backoff_config=BackoffConfig(
        strategy="constant",
        base_delay=5.0,    # 5 second constant delay
        max_tries=3,       # Only 3 attempts
        jitter=None
    )
)
```

#### Custom Exception Handling

```python
from typing import Set, Type

# Only retry on specific exceptions
custom_exceptions: Set[Type[Exception]] = {
    ConnectionError,
    TimeoutError,
    # Add your custom exceptions here
}

custom_config = ExceptionHandlerConfig(
    retryable_exceptions=custom_exceptions,
    backoff_config=BackoffConfig(
        strategy="expo",
        base_delay=1.0,
        max_tries=3
    )
)
```

## Evaluation Modes

### Pointwise Mode (Default)

In pointwise mode, your test function processes each row individually, enabling pipelined evaluation:

```python
@evaluation_test(
    completion_params=[{"model": "gpt-4"}],
    input_dataset=["dataset.jsonl"],
    mode="pointwise"
)
def test_pointwise_evaluation(row: EvaluationRow) -> EvaluationRow:
    """Process each row individually."""
    # Evaluate single row
    score = evaluate_single_row(row)
    row.evaluation_result.score = score
    
    return row
```

**Requirements:**
- Function must have a parameter named `row` of type `EvaluationRow`
- Function must return `EvaluationRow`

### Groupwise Mode

In groupwise mode, your test function processes groups of rollout results from the same original row, useful for comparing different models or parameters:

```python
@evaluation_test(
    completion_params=[
        {"model": "gpt-4", "temperature": 0.1},
        {"model": "gpt-3.5-turbo", "temperature": 0.1},
    ],
    input_dataset=["dataset.jsonl"],
    mode="groupwise"
)
def test_groupwise_evaluation(rows: List[EvaluationRow]) -> List[EvaluationRow]:
    """Process groups of rows from the same original input."""
    # Compare results across different models/parameters
    scores = compare_model_outputs(rows)
    for i, row in enumerate(rows):
        row.evaluation_result.score = scores[i]
    
    return rows
```

**Requirements:**
- Function must have a parameter named `rows` of type `List[EvaluationRow]`
- Function must return `List[EvaluationRow]`
- Must provide at least 2 completion parameters

### All Mode (Batch Mode)

In all mode, your test function receives the entire dataset and processes all rows together:

```python
@evaluation_test(
    completion_params=[{"model": "gpt-4"}],
    input_dataset=["dataset.jsonl"],
    mode="all"
)
def test_all_evaluation(rows: List[EvaluationRow]) -> List[EvaluationRow]:
    """Process all rows together."""
    # Access to full dataset for cross-row analysis
    for row in rows:
        # Evaluate each row
        score = evaluate_single_row(row)
        row.evaluation_result.score = score
    
    return rows
```

**Requirements:**
- Function must have a parameter named `rows` of type `List[EvaluationRow]`
- Function must return `List[EvaluationRow]`

## Threshold Configuration

You can set thresholds for test success using the `passed_threshold` parameter:

```python
# Simple threshold (just success rate)
@evaluation_test(
    completion_params=[{"model": "gpt-4"}],
    passed_threshold=0.8
)

# Advanced threshold with standard error
@evaluation_test(
    completion_params=[CompletionParams(model="gpt-4")],
    passed_threshold={
        "success": 0.8,
        "standard_error": 0.05
    }
)

# Using EvaluationThreshold object
from eval_protocol.models import EvaluationThreshold

@evaluation_test(
    completion_params=[{"model": "gpt-4"}],
    passed_threshold=EvaluationThreshold(success=0.8, standard_error=0.05)
)
```

## Multiple Runs and Aggregation

Set `num_runs > 1` to run multiple evaluations and aggregate results:

```python
@evaluation_test(
    completion_params=[{"model": "gpt-4"}],
    input_dataset=["dataset.jsonl"],
    num_runs=5,
    aggregation_method="mean"
)
def test_with_multiple_runs(rows: List[EvaluationRow]) -> List[EvaluationRow]:
    # This function will be called 5 times
    # Results will be aggregated using the mean
    return rows
```

## Environment Variables

The decorator supports several environment variables for configuration:

- `EP_MAX_DATASET_ROWS`: Override `max_dataset_rows` parameter. Applies to both datasets and `input_messages` (slices to first N rows).
- `EP_INPUT_PARAMS_JSON`: JSON object deep-merged into `completion_params`. Example: `{"temperature":0,"extra_body":{"reasoning":{"effort":"low"}}}`.
- `EP_PRINT_SUMMARY`: Set to "1" to print a one-line evaluation summary to stdout.
- `EP_SUMMARY_JSON`: File or directory path to write a JSON summary artifact. See "Summary artifacts" for naming behavior.
- Retry-related environment variables are documented in the [Retries and failure policy](#retries-and-failure-policy) section.

## Return Values

Your test function must return the appropriate type based on the mode:

- **Pointwise mode**: `EvaluationRow`
- **Groupwise mode**: `List[EvaluationRow]`
- **All mode**: `List[EvaluationRow]`

Each returned row should have:
- `evaluation_result.score`: A float between 0 and 1
- Optional `evaluation_result.metrics`: Additional metric scores

## Dataset loading and input formats

- **Datasets (`input_dataset`)**: You can pass a single path or a list of paths. When a list is provided and `combine_datasets=True` (default), files are concatenated into one dataset; when `combine_datasets=False`, each path is parameterized into separate test invocations.
- **Input messages (`input_messages`)**: Accepts either a single row as `List[Message]` or many rows as `List[List[Message]]`. When `EP_MAX_DATASET_ROWS` is set, the list is sliced before parameterization.
- **Dataset adapter (`dataset_adapter`)**: Receives raw JSONL rows and must return `List[EvaluationRow]`.

## Error Handling

The decorator handles errors gracefully:

- Failed rollouts are still evaluated (you can choose to give them a score of 0)
- Assertion errors are logged with status "finished"
- Other exceptions are logged with status "error"
- Summary generation failures don't cause test failures
- For retry behavior and configuration, see [ExceptionHandlerConfig](#exceptionhandlerconfig) and [Retries and failure policy](#retries-and-failure-policy).

## Row IDs and metadata

- Stable `row_id` values are generated for rows missing `row.input_metadata.row_id`, using a deterministic hash of row content. This ensures consistent IDs across processes and runs.
- `EvalMetadata` is created for each evaluation with: `name` (test function name), `description` (docstring), `num_runs`, `aggregation_method`, and threshold info. Its `status` transitions from "running" to "finished" or "error".
- `completion_params` used for a row are recorded in `row.input_metadata.completion_params`.

## Dataset combination and parameterization

- Parameter combinations are generated across `input_dataset`, `completion_params`, `input_messages`, and `evaluation_test_kwargs`.
- Pytest parameter names (in order when present): `dataset_path`, `completion_params`, `input_messages`, `evaluation_test_kwargs`.
- Set `combine_datasets=False` to parameterize each dataset path separately. With `True` (default), multiple paths are combined into a single logical dataset per invocation.

## Summary artifacts

When `EP_SUMMARY_JSON` is set:

- If a directory or a non-`.json` path is provided, a file is written inside with the base name: `"{suite}__{model}__{mode}__runs{num_runs}.json"`, where `suite` is the test function name and `model` is a sanitized slug.
- If a file path is provided, it writes that file. If an "effort" tag is detected in `completion_params` (e.g., `reasoning_effort`), a variant suffixed with `__{effort}` is written instead.
- The summary includes: `suite`, `model`, `agg_score`, `num_runs`, `rows`, optional 95% CI (`agg_ci_low`, `agg_ci_high`) when `aggregation_method="mean"`, and a `timestamp`.

## Retries and failure policy

- Rollouts are retried up to `EP_MAX_RETRY` times.
- Permanent failures are, by default, raised immediately to fail the test. Override with `EP_FAIL_ON_MAX_RETRY=false` to continue and include errored rows (you can score them as 0 in your evaluation).
- Exception handling and retry logic can be customized via `exception_handler_config`.

### Environment Variables

The following environment variables control retry behavior:

- `EP_MAX_RETRY`: Maximum number of retry attempts (default: 0, meaning no retries)
- `EP_FAIL_ON_MAX_RETRY`: Whether to fail the test after max retries (default: "true")

### Custom Retry Configuration

For advanced retry logic, you can provide a custom `ExceptionHandlerConfig`:

```python
from eval_protocol.pytest.exception_config import ExceptionHandlerConfig, BackoffConfig

# Aggressive retry strategy for unreliable networks
aggressive_retry = ExceptionHandlerConfig(
    backoff_config=BackoffConfig(
        strategy="expo",
        base_delay=0.5,  # Start with 0.5s delay
        max_delay=30.0,  # Cap at 30s
        max_tries=10,    # Try up to 10 times
        jitter=None      # No jitter for predictable timing
    )
)

@evaluation_test(
    completion_params=[{"model": "gpt-4"}],
    exception_handler_config=aggressive_retry
)
def test_with_aggressive_retries(rows: List[EvaluationRow]) -> List[EvaluationRow]:
    # Your evaluation logic here
    return rows
```

## [Rollout processors](/reference/rollout-processors)

- A rollout processor turns input rows into completed rows (e.g., by calling a model). The decorator passes a `RolloutProcessorConfig` containing `completion_params`, `mcp_config_path`, `server_script_path`, `max_concurrent_rollouts`, and `steps`.
- Built-ins include:
  - `NoOpRolloutProcessor()`: passes rows through unchanged (useful for offline evaluation of pre-generated outputs).
  - `SingleTurnRolloutProcessor()`: performs a single chat completion via LiteLLM and appends the assistant message.
  - Others (e.g., agent/MCP variants) exist in `eval_protocol.pytest`.

## Direct invocation (dual-mode)

Decorated functions can be called directly in addition to running under pytest:

- Pointwise mode: `await test_fn(row)` or `await test_fn(row=...)`
- Groupwise mode: `await test_fn(rows)` or `await test_fn(rows=[...])`
- All mode: `await test_fn(rows)` or `await test_fn(rows=[...])`

## Examples

### Basic Math Evaluation (Pointwise Mode)

```python
@evaluation_test(
    completion_params=[CompletionParams(model="gpt-4")],
    input_messages=[
        [Message(role="user", content="What is 2 + 2?")]
    ],
    passed_threshold=0.9,
    mode="pointwise"
)
def test_basic_math(row: EvaluationRow) -> EvaluationRow:
    # Simple correctness check
    response = row.messages[-1].content
    if "4" in response:
        row.evaluation_result.score = 1.0
    else:
        row.evaluation_result.score = 0.0
    
    return row
```

### Multi-Model Comparison (All Mode)

```python
@evaluation_test(
    completion_params=[
        {"model": "gpt-4", "temperature": 0.1},
        {"model": "gpt-3.5-turbo", "temperature": 0.1},
        {"model": "claude-3-sonnet", "temperature": 0.1},
    ],
    input_dataset=["reasoning_tasks.jsonl"],
    passed_threshold=0.7,
    num_runs=3,
    mode="all"
)
def test_reasoning_capabilities(rows: List[EvaluationRow]) -> List[EvaluationRow]:
    for row in rows:
        # Complex evaluation logic
        score = evaluate_reasoning_quality(row.messages)
        row.evaluation_result.score = score
        
        # Add additional metrics
        row.evaluation_result.metrics = {
            "clarity": evaluate_clarity(row.messages),
            "correctness": evaluate_correctness(row.messages)
        }
    
    return rows
```

### Groupwise Evaluation for Model Comparison

```python
@evaluation_test(
    completion_params=[
        {"model": "gpt-4", "temperature": 0.1},
        {"model": "gpt-3.5-turbo", "temperature": 0.1},
    ],
    input_dataset=["comparison_tasks.jsonl"],
    mode="groupwise"
)
def test_model_comparison(rows: List[EvaluationRow]) -> List[EvaluationRow]:
    """Compare outputs from different models on the same input."""
    # Group rows by their original input
    for row in rows:
        # Evaluate relative to other models or absolute quality
        score = evaluate_model_output(row.messages, row.input_metadata.completion_params)
        row.evaluation_result.score = score
    
    return rows
```

### Pointwise Evaluation with Custom Dataset

```python
def custom_dataset_adapter(data: List[Dict[str, Any]]) -> List[EvaluationRow]:
    """Convert custom format to EvaluationRows."""
    rows = []
    for item in data:
        messages = [
            Message(role="user", content=item["question"]),
            Message(role="assistant", content=item["answer"])
        ]
        row = EvaluationRow(messages=messages)
        rows.append(row)
    return rows

@evaluation_test(
    completion_params=[{"model": "gpt-4"}],
    input_dataset=["custom_format.jsonl"],
    dataset_adapter=custom_dataset_adapter,
    mode="pointwise"
)
def test_custom_format(row: EvaluationRow) -> EvaluationRow:
    # Process individual row
    score = evaluate_custom_metric(row.messages)
    row.evaluation_result.score = score
    return row
```

### Complete runnable example (offline, no model calls)

This example evaluates pre-generated assistant messages using the no-op rollout processor.

```python
from typing import Any, Dict, List
from eval_protocol.models import EvaluationRow, Message, EvaluateResult
from eval_protocol.pytest.evaluation_test import evaluation_test
from eval_protocol.pytest.default_no_op_rollout_processor import NoOpRolloutProcessor

def adapter(json_rows: List[Dict[str, Any]]) -> List[EvaluationRow]:
    rows: List[EvaluationRow] = []
    for r in json_rows:
        # Expect fields: question, model_answer, ground_truth
        rows.append(
            EvaluationRow(
                messages=[
                    Message(role="user", content=str(r["question"])) ,
                    Message(role="assistant", content=str(r["model_answer"]))
                ],
                ground_truth=str(r.get("ground_truth", ""))
            )
        )
    return rows

@evaluation_test(
    input_dataset=["offline_answers.jsonl"],
    dataset_adapter=adapter,
    completion_params=[{"model": "not-used-offline"}],
    rollout_processor=NoOpRolloutProcessor(),
    mode="all"
)
def test_offline_eval(rows: List[EvaluationRow]) -> List[EvaluationRow]:
    for row in rows:
        pred = (row.get_assistant_messages()[-1].content or "").strip()
        gt = (row.ground_truth or "").strip()
        score = 1.0 if pred == gt else 0.0
        row.evaluation_result = EvaluateResult(score=score, reason="exact match")
    return rows
```

### Complete runnable example (single-turn online via LiteLLM)

Requires `pip install litellm` and provider credentials configured.

```python
from typing import List
from eval_protocol.models import EvaluationRow, Message, EvaluateResult
from eval_protocol.pytest.evaluation_test import evaluation_test
from eval_protocol.pytest.default_single_turn_rollout_process import SingleTurnRolloutProcessor

@evaluation_test(
    input_messages=[[Message(role="user", content="What is 2 + 2?")]],
    completion_params=[{"model": "openai/gpt-4o-mini", "temperature": 0}],
    rollout_processor=SingleTurnRolloutProcessor(),
    passed_threshold=0.8,
    mode="pointwise"
)
def test_online_math(row: EvaluationRow) -> EvaluationRow:
    answer = (row.get_assistant_messages()[-1].content or "").strip()
    score = 1.0 if "4" in answer else 0.0
    row.evaluation_result = EvaluateResult(score=score, reason="contains 4")
    return row
```

## Integration with pytest

The decorator automatically creates pytest-compatible test functions:

```bash
# Run all evaluation tests
pytest test_file.py

# Run specific test
pytest test_file.py::test_math_reasoning

# Run with specific parameters
pytest test_file.py::test_math_reasoning[dataset_path0-completion_params0]
```

## Programmatic Usage

Decorated functions can be called directly in addition to running under pytest. See [Direct invocation (dual-mode)](#direct-invocation-dual-mode) for patterns by mode.

## Best Practices

1. **Clear Documentation**: Always include docstrings explaining what your evaluation measures
2. **Error Handling**: Handle edge cases gracefully and provide meaningful scores for failed rollouts
3. **Metric Design**: Design metrics that are objective and reproducible
4. **Reason**: Include a `reason` field in the `evaluation_result` to explain the score
5. **Threshold Setting**: Set realistic thresholds based on your use case
6. **Multiple Runs**: Use `num_runs > 1` for more reliable results when possible
7. **Resource Management**: Consider `max_concurrent_rollouts` and `max_concurrent_evaluations` based on your system capabilities
8. **Mode Selection**: Choose the appropriate mode for your evaluation needs:
   - Use "pointwise" for simple per-row evaluation
   - Use "groupwise" for comparing multiple models/parameters on the same inputs
   - Use "all" for batch processing with cross-row analysis

## Troubleshooting

### Common Issues

- **"No combinations of parameters found"**: Ensure you provide both `completion_params` and either `input_dataset` or `input_messages`
- **"No model provided"**: Check that your `CompletionParams` includes a `model` field
- **Signature validation errors**: Ensure your function signature matches the mode requirements:
  - Pointwise mode: `def func(row: EvaluationRow) -> EvaluationRow`
  - Groupwise mode: `def func(rows: List[EvaluationRow]) -> List[EvaluationRow]`
  - All mode: `def func(rows: List[EvaluationRow]) -> List[EvaluationRow]`
- **Return type errors**: Verify you're returning the correct type based on your mode
- **"In groupwise mode, you must provide at least 2 completion parameters"**: Groupwise mode requires multiple completion parameters to compare

### Debug Tips

- Set `EP_PRINT_SUMMARY=1` to see evaluation results in console
- Use `EP_SUMMARY_JSON` to save detailed results to a file
- Check the generated pytest parameterization for complex setups
- Use `max_dataset_rows` to limit dataset size during development
- Monitor `max_concurrent_rollouts` and `max_concurrent_evaluations` for performance tuning