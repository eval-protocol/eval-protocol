---
title: Getting Started
description: 'Learn the fundamentals of reward functions and evaluation workflows'
---

# Getting Started with Reward Functions

This guide introduces the core concepts of Eval Protocol and walks you through creating your first reward function.

## What is a Reward Function?

A reward function is a Python function that evaluates the quality of AI model responses. It takes a conversation history and returns a score along with detailed metrics.

### Key Components

1. **Input**: Messages from a conversation (user and assistant)
2. **Processing**: Your custom evaluation logic
3. **Output**: Structured result with score and metrics

## Core Concepts

### 1. The `@reward_function` Decorator

The decorator transforms your function into a deployable evaluator:

```python
from eval_protocol import reward_function
from eval_protocol.models import EvaluateResult

@reward_function
def my_evaluator(messages, **kwargs):
    # Your evaluation logic here
    return EvaluateResult(score=1.0, reason="Perfect!")
```

### 2. Message Structure

Messages follow a standardized format:

```python
messages = [
    {"role": "user", "content": "What is 2+2?"},
    {"role": "assistant", "content": "2+2 equals 4."}
]
```

### 3. Evaluation Results

All reward functions return an `EvaluateResult`:

```python
EvaluateResult(
    score=0.85,                    # Primary score (0.0 to 1.0)
    reason="Response was accurate but brief",
    metrics={                      # Additional metrics
        "accuracy": MetricResult(
            score=1.0,
            success=True,
            reason="Answer is correct"
        ),
        "length": MetricResult(
            score=0.7,
            success=True,
            reason="Response could be more detailed"
        )
    }
)
```

## Step-by-Step Tutorial

### Step 1: Create Your First Reward Function

```python
from eval_protocol import reward_function
from eval_protocol.models import EvaluateResult, MetricResult
from typing import List, Dict, Any

@reward_function
def helpfulness_reward(
    messages: List[Dict[str, Any]], 
    **kwargs: Any
) -> EvaluateResult:
    """Evaluate how helpful an assistant's response is."""
    
    # Get the assistant's response
    assistant_message = messages[-1]
    response = assistant_message.get("content", "")
    
    # Simple helpfulness metrics
    word_count = len(response.split())
    has_examples = "example" in response.lower()
    has_explanation = any(word in response.lower() for word in ["because", "since", "due to"])
    
    # Calculate scores
    length_score = min(word_count / 50.0, 1.0)  # Max score at 50 words
    example_score = 1.0 if has_examples else 0.0
    explanation_score = 1.0 if has_explanation else 0.0
    
    # Combined score
    final_score = (length_score * 0.4 + example_score * 0.3 + explanation_score * 0.3)
    
    return EvaluateResult(
        score=final_score,
        reason=f"Helpfulness score based on length ({word_count} words), examples ({has_examples}), and explanations ({has_explanation})",
        metrics={
            "length": MetricResult(
                score=length_score,
                success=word_count >= 20,
                reason=f"Response has {word_count} words"
            ),
            "examples": MetricResult(
                score=example_score,
                success=has_examples,
                reason=f"Contains examples: {has_examples}"
            ),
            "explanation": MetricResult(
                score=explanation_score,
                success=has_explanation,
                reason=f"Contains explanations: {has_explanation}"
            )
        }
    )
```

### Step 2: Test Your Function

```python
# Test data
test_messages = [
    {"role": "user", "content": "How do I bake a cake?"},
    {"role": "assistant", "content": "To bake a cake, you need flour, sugar, eggs, and butter. For example, a basic vanilla cake requires 2 cups of flour because the flour provides structure to the cake."}
]

# Evaluate
result = helpfulness_reward(test_messages)
print(f"Score: {result.score}")
print(f"Reason: {result.reason}")
for metric_name, metric in result.metrics.items():
    print(f"{metric_name}: {metric.score} ({'✓' if metric.success else '✗'})")
```

### Step 3: Create Sample Data

Create a `samples.jsonl` file:

```json
{"messages": [{"role": "user", "content": "How do I bake a cake?"}, {"role": "assistant", "content": "To bake a cake, you need flour, sugar, eggs, and butter. For example, a basic vanilla cake requires 2 cups of flour because the flour provides structure."}]}
{"messages": [{"role": "user", "content": "What is Python?"}, {"role": "assistant", "content": "Python is a programming language."}]}
```

### Step 4: Preview Your Evaluation

```bash
reward-protocol preview --metrics-folders "helpfulness=./path/to/your/module" --samples ./samples.jsonl
```

## Best Practices

### 1. Handle Edge Cases

```python
@reward_function
def robust_reward(messages, **kwargs) -> EvaluateResult:
    # Check for empty messages
    if not messages:
        return EvaluateResult(score=0.0, reason="No messages provided")
    
    # Check for assistant response
    if messages[-1].get("role") != "assistant":
        return EvaluateResult(score=0.0, reason="No assistant response found")
    
    # Your evaluation logic
    # ...
```

### 2. Use Ground Truth When Available

```python
@reward_function
def accuracy_reward(messages, ground_truth=None, **kwargs) -> EvaluateResult:
    response = messages[-1].get("content", "")
    
    if ground_truth:
        expected = ground_truth.get("answer", "")
        is_correct = response.lower().strip() == expected.lower().strip()
        return EvaluateResult(
            score=1.0 if is_correct else 0.0,
            reason=f"Expected: {expected}, Got: {response}"
        )
    
    # Fallback evaluation without ground truth
    return EvaluateResult(score=0.5, reason="No ground truth available")
```

### 3. Provide Clear Metrics

```python
@reward_function
def detailed_reward(messages, **kwargs) -> EvaluateResult:
    # ... evaluation logic ...
    
    return EvaluateResult(
        score=final_score,
        reason="Clear explanation of the overall score",
        metrics={
            "metric1": MetricResult(
                score=score1,
                success=score1 > threshold,
                reason="Specific reason for this metric"
            ),
            "metric2": MetricResult(
                score=score2,
                success=condition_met,
                reason="Another specific reason"
            )
        }
    )
```

## Common Patterns

### 1. Text Analysis

```python
import re

@reward_function
def text_quality_reward(messages, **kwargs) -> EvaluateResult:
    response = messages[-1].get("content", "")
    
    # Grammar check (simple)
    sentence_count = len(re.findall(r'[.!?]+', response))
    word_count = len(response.split())
    avg_sentence_length = word_count / max(sentence_count, 1)
    
    # Scoring logic
    # ...
```

### 2. Format Validation

```python
@reward_function
def format_reward(messages, **kwargs) -> EvaluateResult:
    response = messages[-1].get("content", "")
    
    # Check for required format
    has_code_block = "```" in response
    has_explanation = len(response.split('\n')) > 3
    
    # Scoring logic
    # ...
```

### 3. Domain-Specific Evaluation

```python
@reward_function
def math_reward(messages, **kwargs) -> EvaluateResult:
    response = messages[-1].get("content", "")
    
    # Extract numerical answer
    import re
    numbers = re.findall(r'\d+\.?\d*', response)
    
    # Mathematical validation
    # ...
```

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Reward Function Anatomy"
    icon="microscope"
    href="/developer-guide/reward-function-anatomy"
  >
    Deep dive into function structure
  </Card>
  <Card
    title="Evaluation Workflows"
    icon="workflow"
    href="/developer-guide/evaluation-workflows"
  >
    End-to-end evaluation processes
  </Card>
  <Card
    title="Examples"
    icon="code"
    href="/examples/overview"
  >
    Real-world implementations
  </Card>
  <Card
    title="API Reference"
    icon="book"
    href="/api-reference/overview"
  >
    Detailed API documentation
  </Card>
</CardGroup>